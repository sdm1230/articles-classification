YouTube Says It Will Ban Misleading Election-Related Content
By Davey AlbaBOSTON — YouTube said on Monday that it planned to remove misleading election-related content that can cause “serious risk of egregious harm,” the first time the video platform has comprehensively laid out how it will handle such political videos and viral falsehoods.The Google-owned site, which previously had several different policies in place that addressed false or misleading content, rolled out the full plan on the day of the Iowa caucuses, when voters will begin to indicate their preferred Democratic presidential candidate.“Over the last few years, we’ve increased our efforts to make YouTube a more reliable source for news and information, as well as an open platform for healthy political discourse,” Leslie Miller, the vice president of government affairs and public policy at YouTube, said in a blog post. She added that YouTube would be enforcing its policies “without regard to a video’s political viewpoint.”The move is the latest attempt by tech companies to grapple with online disinformation, which is likely to ramp up ahead of the November election. Last month, Facebook said it would remove videos that were altered by artificial intelligence in ways meant to mislead viewers, though it has also said it would allow political ads and would not police them for truthfulness. Twitter has banned political ads entirely and has said it will largely not muzzle political leaders’ tweets, though it may denote them differently.In dealing with election-related disinformation, YouTube faces a formidable task. More than 500 hours of video a minute is uploaded to the site. The company has also grappled with concerns that its algorithms may push people toward radical and extremist views by showing them more of that type of content.In its blog post on Monday, YouTube said it would ban videos that gave users the wrong voting date or those that spread false information about participating in the census. It said it would also remove videos that spread lies about a political candidate’s citizenship status or eligibility for public office. One example of a serious risk could be a video that was technically manipulated to make it appear that a government official was dead, YouTube said.The company added that it would terminate YouTube channels that tried to impersonate another person or channel, conceal their country of origin, or hide an association with the government. Likewise, videos that boosted the number of views, likes, comments and other metrics with the help of automated systems would be taken down.YouTube is likely to face questions about whether it applies these policies consistently as the election cycle ramps up. Like Facebook and Twitter, YouTube faces the challenge that there is often no “one size fits all” method of determining what amounts to a political statement and what kind of speech crosses the line into public deception.Graham Brookie, the director of the Atlantic Council’s Digital Forensic Research Lab, said that while the policy gave “more flexibility” to respond to disinformation, the onus would be on YouTube for how it chose to respond, “especially in defining the authoritative voices YouTube plans to upgrade or the thresholds for removal of manipulated videos like deepfakes.”Ivy Choi, a YouTube spokeswoman, said a video’s context and content would determine whether it was taken down or allowed to remain. She added that YouTube would focus on videos that were “technically manipulated or doctored in a way that misleads users beyond clips taken out of context.”As an example, she cited a video that went viral last year of Speaker Nancy Pelosi, a Democrat from California. The video was slowed down to make it appear as if Ms. Pelosi were slurring her words. Under YouTube’s policies, that video would be taken down because it was “technically manipulated,” Ms. Choi said.But a video of former Vice President Joseph R. Biden Jr. responding to a voter in New Hampshire, which was cut to wrongly suggest that he made racist remarks, would be allowed to stay on YouTube, Ms. Choi said.She said deepfakes — videos that are manipulated by artificial intelligence to make subjects look a different way or say words they did not actually say — would be removed if YouTube determined they had been created with malicious intent. But whether YouTube took down parody videos would again depend on the content and the context in which they were presented, she said.Renée DiResta, the technical research manager for the Stanford Internet Observatory, which studies disinformation, said YouTube’s new policy was trying to address “what it perceives to be a newer form of harm.”“The downside here, and where missing context is different than a TV spot with the same video, is that social channels present information to people most likely to believe them,” Ms. DiResta added.Updated May 25, 2020